# 项目背景

项目主要包括三个方面：

1. 数仓搭建
2. 实时计算
3. 离线计算

手机已经是人们生活中不可缺少的重要通信、咨询、娱乐的工具。手机在带来方便的同时也带来了烦恼，就是信号质量差手机上网速度慢等问题。针对这一问题，如何为用户提供更好的服务是需要重点关注的问题。
以中国移动为例，全国基站数量462万个，5G基站38万个（全国约70万个），21明年预计新建60万，总数约100万个。每个基站覆盖距离，5-10公里(2G)，2-5公里(3G)，1-3公里(4G)，500-800米(5G)。相同的环境下建站的成本翻倍。如何优化无线网络，降低建设成本提升用户体验就变得至关重要

# 数据来源

## 日志数据

每个用户的手机会给上报基站的信息，包含用户的信号强度，上下行流量，网络制式，运营商信息，经纬度位置信息。

1. 确定每个小区需要集群规模（假设每台服务器8T硬盘，128G内存）每10ms资源调度一次，1s调度100次，每次参与调度的用户最多120个，
2. 一个基站1s收到的信息做多是120×100=12000条日志，每天产生日志约24×3600×12000≈10亿条，每条日志1K左右，每个基站每天日志大小1T；
3. ODS层采用LZO压缩，10:1压缩，需要100G；DWD层需要100G；DWS轻度数据聚合（不压缩），大概500G；保存3个副本，（100G+100G+500G）×3=2100G

## Kafka中的数据

1. 每天1T×2（副本数）=2T
2. 保存3天3×2T=6T
3. 预留30%，6T/0.7=9T

# 数仓

主要分四层：

	• ODS（Operation Data Store）原始数据层，直接加载原始日志、数据，数据保持原貌不变
	• DWD（Data Warehose Detail）明细数据层，结构和ODS保持一致，对ODS进行数据清洗ETL（去除空置，脏数据，超出极限范围的数据）
	• DWS（Data Warehouse Service）服务数据层，以DWD为基础，进行轻度汇总
分四层的原因：为了隔离数据然后还能复用上一层计算的结果；另一方面为了数据备份

1. 首先是ods层 : ods层的数据来源主要是log⽇志和业务系统的数据,业务系统的数据值得就是从Mysql数据库导⼊过来的数据,log⽇志主要是通过采集系统采集过来的;

  <img src="面试项目介绍.assets/大数据面试吹牛草稿 V1.jpg" alt="大数据面试吹牛草稿 V1" style="zoom: 67%;" />

## Flume

<img src="面试项目介绍.assets/大数据面试吹牛草稿 V1-1608551694741.jpg" alt="大数据面试吹牛草稿 V1" style="zoom: 33%;" />

1. Flume在1.7以后提供了⼀个T ailDirSource⽤来⽀持多⽬录和断点续传功能;

   (1) 断点续传主要保证在服务器挂掉的情况下,再次启动服务数据不会丢失的问题;其原理就是在底层维护了⼀个off set偏移量(也就是每次读取⽂件的偏移量)Flume会通过这个偏移量来找到上次⽂件读取的位置从⽽实现了断点续传的功能;

   (2) 在1.6以前这个实现断点续传是需要⼿⼯维护这个偏移量的会⽐较麻烦;

2. Channel的种类⽐较多主要有:

   (1) MemoryChannel : 数据放在内存中,会在Flume宕机的时候丢失数据,可以⽤在对数据安全性要求没有那么⾼的场景中⽐如⽇志数据;

   (2) FileChannel : 不会丢失数据,因为数据是放在磁盘上边的⽽且⽀持多⽬录配置可以提⾼写⼊的性能,同时因为有落盘的操作所以效率⽐较低,适合⽤在对数据安全性要求⽐较⾼的场景⽐如⾦融类的数据;

   (3) Kaf kaChannel : 主要是为了对接Kaf ka,使⽤这个可以节省Sink组件也可以提升效率的,我们项⽬中使⽤的就是这个Channel,因为下⼀层是使⽤Kafka来传递消息的;

在Flume这⼀层我们还做了⼀个拦截器,主要是对收集到的⽇志做了⼀层过滤,因为⽇志在后台都是以Json的格式进⾏存储的,在拦截器⾥边对格式不合法的Json进⾏了⼀次简单清洗;

还做了⼀个分类型的拦截器,在这个拦截器⾥边我们对数据进⾏类型的区分,主要是做了⼀个打标签的功能对不同的⽇志数据打上不同的标签,然后通过后续的选择器Mult iplexing将不同标签的数据放到不同的t opic⾥边,⽅便下游对数据进⾏处理;



## Kafka

下游采用Kafka作为消息队列传输消息，使用Kafka的原因是Kafka的高吞吐量以及可以对数据按照不同的topic进行分区，一部分数据放到HDFS作离线处理，一部分通过SparkStreaming做实时批处理

为什么选择Kafka作为消息队列：主要是考虑到高吞吐量

Kafka为什么可以这么快：

1. 生产者发送数据是按照批而不是一条一条的发送

2. 生产者生产一条消息会进入到一个拦截器，从拦截器里出来后进入到序列化器，在序列化器里将数据转换成一个二进制流的形式放入到Broker中

3. 经过序列化后会经过分区器，Kafka的分区器是hash分区器：有三种分区器

   （a）数据带有key，则根据key计算hash

   （b）数据本身带有分区号

   （c）还有个叫累加器的东西，这个分区器其实是一个hashmap，key是分区号，value是一个双端队列，如果发送失败，则会进入队列再次发送

所需Kafka数量=2×（峰值生产速度×副本数/100）+1

Kafka内存一般设置为4-5G

**Kafka架构：**

<img src="面试项目介绍.assets/大数据面试吹牛草稿 V1-1608553723702.jpg" alt="大数据面试吹牛草稿 V1" style="zoom:50%;" />

Kafka主要包含Broker和Consumer，其中Broker是集群的，一个Broker可以包含多个Topic，topic主要用来存储数据同时分为多个partition，用来增加数据传递的并发度，Broker和Consumer的信息都存储在Zookeeper 中

生产者将消息发送到Broker的过程可能会出现消息的重复或者丢失的情况，主要是ACK的配置决定的：

1. ACK=-1这种情况是不会丢失数据的,因为Broker包含T opic,T opic⼜包含Part it ion,⽽Part it ion⾥边还有副本,这⾥就会有Leader和Follower的概念,此时⽣产者发送消息后Leader会等待所有Follower的响应后才会向⽣产者响应,此时是不会丢失数据的,但是会有数据重复的问题这个问题可以在下游消费数据的时候进⾏处理⼀般都是采⽤去重的操作;
2. Ack = 0 : 此时相当于是消息的异步发送,⽣产者发送消息以后不会等待Leader这边的任何响应,如果在发送消息后系统发⽣停电或者宕机此时是会丢失数据的但是这种机制的特点是效率特别⾼;
3. Ack = 1 : 此时⽣产者会等待Leader响应,但是Leader响应的时间是在接受到数据后⽽不是所有的Follower成功后继续你那个响应,如果在Leader刚响应并且Follower没有来得及进⾏同步,此时服务器宕机也会丢失数据的;

ISR副本同步队列,在这个队列⾥边包含了Leader和Follower,主要解决的问题就是Leader挂了以后谁来做Leader的问题

Flume往Kafka传递消息时，会遇到小文件问题，Flume默认根据event的数量生成文件，多少个event就产生多少个文件，小文你见过多产生的危害：

1. 对namenode占用大量存储空间，影响其性能
2. 影响计算性能，因为每个小文件单独生成一个map任务，小文件过多会导致maptask任务过多从而影响计算的性能

解决项文件过多的方法：

1. rollIntercal，设置文件滚动生成时间
2. rollSize，设置文件滚动大小
3. rollCount按照滚动文件个数

## 数仓

<img src="面试项目介绍.assets/大数据面试吹牛草稿 V1-1608556723206.jpg" alt="大数据面试吹牛草稿 V1" style="zoom:80%;" />

### ODS

1. 数据缓冲层，只涉及一个字段进行存储，方便管理和备份
2. 采用LZO压缩
3. 按照日期进行分区

### DWD

1. 多上一层数据进行分解
2. 数据脱敏和过滤
3. 对数据进行降维降级

### DWS

## Hive

# Spark



